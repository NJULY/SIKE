# Bidirectional Transferable Representation Learning for Source-Incremental Knowledge Graphs

> Over the years, massive knowledge graphs (KGs) have been constructed one after another, forming continuously expanding sources of structured knowledge. KG representation learning, aimed at embedding entities and relations into a low-dimensional vector space, has shown competitive performance in many knowledge-driven applications. Existing models primarily learn representations on a single KG, ignoring the source-incremental reality, which necessitates efficient representation learning and effective knowledge transfer across KGs. In this paper, we investigate the representation learning of source-incremental KGs and propose a novel model. We design an incremental learning pipeline using a frozen pre-trained language model and KG-specific adapters to learn knowledge and avoid catastrophic forgetting in a sequence of multi-source KGs. To take advantage of the complementary facts of multi-source KGs, we also propose a forward knowledge transfer method to transfer knowledge among KGs and a cross-modal distillation method to distill structural knowledge into text-based representations. To simulate the real-world scenario, we choose three widely used KGs to create a new dataset for evaluating source-incremental KG embedding. The experimental results show that our model can continually learn representations for emerging KGs and benefit from the transferable knowledge in previous KGs and embeddings.

## Dependencies:
- pytorch>=1.10
- transformers>=4.10
